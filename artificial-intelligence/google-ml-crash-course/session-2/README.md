# Session 2

## 4. Reducing Loss

### Gradient Descent
- [Visualized interaction](https://developers.google.com/machine-learning/crash-course/fitter/graph)
- Convex: start anywhere
- Foreshadowing: strong dependency on initial values

### Efficiency of Gradient Descent
- Stochastic Gradient Descent: one at a time
- Mini-Batch Gradient Descent: batch of 10-1000
    - Loss & gradient are averaged over the batch